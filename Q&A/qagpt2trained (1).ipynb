{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9365100,"sourceType":"datasetVersion","datasetId":5678899}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install chardet","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:41.445509Z","iopub.execute_input":"2024-09-15T07:39:41.445858Z","iopub.status.idle":"2024-09-15T07:39:56.818002Z","shell.execute_reply.started":"2024-09-15T07:39:41.445822Z","shell.execute_reply":"2024-09-15T07:39:56.816985Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: chardet\nSuccessfully installed chardet-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndef convert_csv_to_text(input_csv, output_txt):\n    # Load CSV without headers and handle encoding issues\n    try:\n        df = pd.read_csv(input_csv, header=None, encoding='ISO-8859-1')  # Handle special characters\n    except pd.errors.ParserError:\n        # Try loading with tab separator if default fails\n        df = pd.read_csv(input_csv, header=None, delimiter='\\t', encoding='ISO-8859-1')\n\n    print(df.head())  # Debugging: Show the first few rows to check the structure\n\n    with open(output_txt, 'w', encoding='utf-8') as f:  # Ensure the output is UTF-8\n        current_question = None\n        for index, row in df.iterrows():\n            try:\n                # Convert each value to string and handle NaN values\n                question = str(row[0]) if not pd.isna(row[0]) else ''\n                answer = str(row[1]) if not pd.isna(row[1]) else ''\n\n                # Only proceed if both question and answer are non-empty\n                if question and answer:\n                    # Remove 'Question :' and 'Answer :' prefixes\n                    question = question.replace('Question :', '').strip()\n                    answer = answer.replace('Answer :', '').strip()\n\n                    if question == current_question:\n                        f.write(f\"Answer: {answer}\\n\")\n                    else:\n                        current_question = question\n                        f.write(f\"Question: {question}\\n\")\n                        f.write(f\"Answer: {answer}\\n\")\n            except KeyError as e:\n                print(f\"Error accessing row data: {e}\")\n                print(row)\n\n# Convert CSV files\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_train_1.csv', '/kaggle/working/train.txt')\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_dev_1.csv', '/kaggle/working/val.txt')\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_test_1.csv', '/kaggle/working/test.txt')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:56.820153Z","iopub.execute_input":"2024-09-15T07:39:56.820440Z","iopub.status.idle":"2024-09-15T07:39:57.379937Z","shell.execute_reply.started":"2024-09-15T07:39:56.820408Z","shell.execute_reply":"2024-09-15T07:39:57.378722Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"                                                   0  \\\n0  Question :What is the difference between conca...   \n1  Question :What is the difference between conca...   \n2  Question :Why are derivatives substracted from...   \n3  Question :Why are derivatives substracted from...   \n4  Question :Describe a process/pipeline for gene...   \n\n                                                   1  \n0  Answer :Concatenation combines two tensors by ...  \n1  Answer :Concatenation is often used to combine...  \n2  Answer :The derivative of the loss function at...  \n3  Answer :The intuition behind adjusting the wei...  \n4  Answer :Select a pretrained model suitable for...  \n                                                   0  \\\n0  Question : Does the maximum value of 'n' in th...   \n1                          Question : How CNN works?   \n2  Question : How is NMT trained? Is it common to...   \n3  Question : What is the process of learning POS...   \n4  Question : how to handle multi lingual situati...   \n\n                                                   1  \\\n0  Answer1 :The maximum value of n in an n-gram m...   \n1  Answer1 :Convolutional Neural Networks (CNNs) ...   \n2  Answer1 :Neural Machine Translation (NMT) is t...   \n3  Answer1 :The process of learning POS tags invo...   \n4  Answer1 :Handling multilingual situations in N...   \n\n                                                   2  \n0  Answer2 :The maximum value of 'n' in an n-gram...  \n1  Answer2 :CNN is a type of neural network that ...  \n2  Answer2 :Yes, training NMT models involves usi...  \n3  Answer2 :POS tags are learned by training mach...  \n4  Answer2 :It involves techniques such as langua...  \n                                                   0  \\\n0  Question :How we can effectively convert 2D im...   \n1  Question :Can we utilize an autoencoder to per...   \n2  Question :What is NLP's current biggest challe...   \n3  Question :Which problems cannot be solved by N...   \n4            Question :Is scaling necessary for SVM?   \n\n                                                   1  \\\n0  Answer1 :Converting images to 1D data may not ...   \n1  Answer1 :Yes, autoencoders can be applied to n...   \n2  Answer1 :The main challenges of NLP is finding...   \n3  Answer1 :While neural networks have shown grea...   \n4  Answer1 :Yes, scaling the input data is genera...   \n\n                                                   2  \n0  Answer2 :To effectively convert 2D images to 1...  \n1  Answer2 :Yes, autoencoders can be used for dim...  \n2  Answer2 :NLP models struggle with tasks that r...  \n3  Answer2 :Neural networks are powerful, but the...  \n4  Answer2 :Scaling the input data is advisable w...  \n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\ndef augment_data(input_csv, output_csv):\n    df = pd.read_csv(input_csv, header=None, encoding='ISO-8859-1')\n    augmented_rows = []\n    \n    for _, row in df.iterrows():\n        question = row[0]\n        answers = [row[i] for i in range(1, len(row)) if pd.notna(row[i])]\n        \n        if len(answers) > 1:\n            augmented_answers = [answer.replace('Answer:', 'Paraphrased Answer:') for answer in answers]\n            for ans in augmented_answers:\n                augmented_rows.append([question] + [ans])\n    \n    augmented_df = pd.DataFrame(augmented_rows, columns=['Question', 'Answer'])\n    augmented_df.to_csv(output_csv, index=False, header=False)\n\n# Augment the training data\naugment_data('/kaggle/input/qa-dataset/aiml_qa_train_1.csv', '/kaggle/working/augmented_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:57.381108Z","iopub.execute_input":"2024-09-15T07:39:57.381387Z","iopub.status.idle":"2024-09-15T07:39:57.500824Z","shell.execute_reply.started":"2024-09-15T07:39:57.381356Z","shell.execute_reply":"2024-09-15T07:39:57.499955Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade datasets transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:57.503739Z","iopub.execute_input":"2024-09-15T07:39:57.504570Z","iopub.status.idle":"2024-09-15T07:40:23.942839Z","shell.execute_reply.started":"2024-09-15T07:39:57.504536Z","shell.execute_reply":"2024-09-15T07:40:23.941665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m665.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, datasets\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.21.0\n    Uninstalling datasets-2.21.0:\n      Successfully uninstalled datasets-2.21.0\nSuccessfully installed datasets-3.0.0 transformers-4.44.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# Load the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\ndef tokenize_function(data):\n    encodings = tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=512)\n    encodings['labels'] = encodings['input_ids']\n    return encodings\n\n# Load and tokenize datasets\ntrain_dataset = load_dataset('text', data_files='/kaggle/working/train.txt')['train']\nval_dataset = load_dataset('text', data_files='/kaggle/working/val.txt')['train']\ntest_dataset = load_dataset('text', data_files='/kaggle/working/test.txt')['train']\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nval_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:40:23.954938Z","iopub.execute_input":"2024-09-15T07:40:23.955192Z","iopub.status.idle":"2024-09-15T07:40:54.910243Z","shell.execute_reply.started":"2024-09-15T07:40:23.955164Z","shell.execute_reply":"2024-09-15T07:40:54.909436Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6d8f626bfe4bcca08bc583523c7328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c600dd8c18241a8806309498d6a7324"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504cee2e87dd483885c64ed5b7073178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17e57c874f64f90a4498f4278d21be6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651164c888324be9b4ba315962631f6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee30a5e43b5941e9a61aa38627c9c11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cc057971be44e296d31580eedb6615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c594460b59f4b06afbd6440d749e267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1dd6dd2c8a4abfb8ce57eef1fe0351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76fbb867348f4905be3c193e655dfe55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1091 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54daf5e7d931473eba4523a63d1bb940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f0d310cc8d490b9699c325228ae35f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42c7a8ff24a41d3a70365eb9aedacd1"}},"metadata":{}}]},{"cell_type":"code","source":"# Define training arguments with adjusted parameters\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/output\",\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=5,          # Experiment with fewer epochs\n    learning_rate=5e-5,          # Adjust learning rate\n    per_device_train_batch_size=8,  # Adjust batch size\n    per_device_eval_batch_size=8,   # Adjust batch size\n    logging_dir=\"/kaggle/working/logs\",\n    save_total_limit=1,\n    save_steps=500,             # Save less frequently\n    logging_steps=50,           # Log more frequently\n    warmup_steps=500,           # Adjust warmup steps\n    weight_decay=0.01,\n    max_grad_norm=1.0,          # Gradient clipping\n    fp16=True,                  # Use mixed precision if supported\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\n# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:40:54.911276Z","iopub.execute_input":"2024-09-15T07:40:54.911884Z","iopub.status.idle":"2024-09-15T07:50:51.777510Z","shell.execute_reply.started":"2024-09-15T07:40:54.911847Z","shell.execute_reply":"2024-09-15T07:50:51.776592Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240915_074116-5q038qth</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth' target=\"_blank\">/kaggle/working/output</a></strong> to <a href='https://wandb.ai/abcdkgf993-aiml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abcdkgf993-aiml/huggingface' target=\"_blank\">https://wandb.ai/abcdkgf993-aiml/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth' target=\"_blank\">https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [345/345 09:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.914400</td>\n      <td>0.246059</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.270700</td>\n      <td>0.204762</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.158800</td>\n      <td>0.199218</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.145500</td>\n      <td>0.196333</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.143800</td>\n      <td>0.195846</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=345, training_loss=1.0048119876695716, metrics={'train_runtime': 593.6304, 'train_samples_per_second': 9.189, 'train_steps_per_second': 0.581, 'total_flos': 1425348034560000.0, 'train_loss': 1.0048119876695716, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/saved_model\")\ntokenizer.save_pretrained(\"/kaggle/working/saved_model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:50:51.779081Z","iopub.execute_input":"2024-09-15T07:50:51.779690Z","iopub.status.idle":"2024-09-15T07:50:53.126413Z","shell.execute_reply.started":"2024-09-15T07:50:51.779637Z","shell.execute_reply":"2024-09-15T07:50:53.125318Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/vocab.json',\n '/kaggle/working/saved_model/merges.txt',\n '/kaggle/working/saved_model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r /kaggle/working/saved_model_qa.zip /kaggle/working/saved_model","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:50:53.127991Z","iopub.execute_input":"2024-09-15T07:50:53.128446Z","iopub.status.idle":"2024-09-15T07:51:21.345105Z","shell.execute_reply.started":"2024-09-15T07:50:53.128395Z","shell.execute_reply":"2024-09-15T07:51:21.343958Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/saved_model/ (stored 0%)\n  adding: kaggle/working/saved_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/saved_model/config.json (deflated 52%)\n  adding: kaggle/working/saved_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/saved_model/merges.txt (deflated 53%)\n  adding: kaggle/working/saved_model/vocab.json (deflated 68%)\n  adding: kaggle/working/saved_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/saved_model/tokenizer_config.json (deflated 55%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom datasets import load_dataset\n\n# Load the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/saved_model')\nmodel = GPT2LMHeadModel.from_pretrained('/kaggle/working/saved_model')\n\n# Load the test dataset\ntest_dataset = load_dataset('text', data_files='/kaggle/working/test.txt')['train']\n\n# Print column names to understand the dataset structure\nprint(test_dataset.column_names)\n\n# Tokenize the test dataset\ndef tokenize_function(data):\n    encodings = tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=512)\n    encodings['labels'] = encodings['input_ids']\n    return encodings\n\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:08:45.284421Z","iopub.execute_input":"2024-09-15T08:08:45.285222Z","iopub.status.idle":"2024-09-15T08:08:48.033204Z","shell.execute_reply.started":"2024-09-15T08:08:45.285180Z","shell.execute_reply":"2024-09-15T08:08:48.032192Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['text']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fe962326ef4b96ab375c6fb3f98a0c"}},"metadata":{}}]},{"cell_type":"code","source":"print(test_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:08:53.123740Z","iopub.execute_input":"2024-09-15T08:08:53.124116Z","iopub.status.idle":"2024-09-15T08:08:53.131215Z","shell.execute_reply.started":"2024-09-15T08:08:53.124081Z","shell.execute_reply":"2024-09-15T08:08:53.130030Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom datasets import load_dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:03.558976Z","iopub.execute_input":"2024-09-15T08:09:03.559405Z","iopub.status.idle":"2024-09-15T08:09:03.566623Z","shell.execute_reply.started":"2024-09-15T08:09:03.559352Z","shell.execute_reply":"2024-09-15T08:09:03.565649Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def predict(example):\n    input_ids = torch.tensor(example['input_ids']).unsqueeze(0)  # Add batch dimension\n    attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0)  # Add batch dimension\n\n    prediction = generate_predictions(input_ids, attention_mask, model, tokenizer)\n    return {\"predictions\": prediction}\n\ndef generate_predictions(input_ids, attention_mask, model, tokenizer, max_new_tokens=50):\n    outputs = model.generate(\n        input_ids,\n        attention_mask=attention_mask,  # Include attention mask\n        max_new_tokens=max_new_tokens,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:07.199848Z","iopub.execute_input":"2024-09-15T08:09:07.200781Z","iopub.status.idle":"2024-09-15T08:09:07.210793Z","shell.execute_reply.started":"2024-09-15T08:09:07.200661Z","shell.execute_reply":"2024-09-15T08:09:07.209468Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Apply prediction function to the test dataset\ntest_predictions = test_dataset.map(predict, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:11.879861Z","iopub.execute_input":"2024-09-15T08:09:11.880249Z","iopub.status.idle":"2024-09-15T08:15:08.003855Z","shell.execute_reply.started":"2024-09-15T08:09:11.880212Z","shell.execute_reply":"2024-09-15T08:15:08.002855Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc57b00864e40a599c33148d5db3b8e"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:15:08.005940Z","iopub.execute_input":"2024-09-15T08:15:08.006631Z","iopub.status.idle":"2024-09-15T08:15:23.943562Z","shell.execute_reply.started":"2024-09-15T08:15:08.006579Z","shell.execute_reply":"2024-09-15T08:15:23.942352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b46d53143ea3b5c13ecc633eaf160b91ea91ee091155a19326ce8aec2d3797fe\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:19:46.239573Z","iopub.execute_input":"2024-09-15T08:19:46.240042Z","iopub.status.idle":"2024-09-15T08:20:01.130416Z","shell.execute_reply.started":"2024-09-15T08:19:46.239999Z","shell.execute_reply":"2024-09-15T08:20:01.129123Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m754.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu, evaluate\nSuccessfully installed evaluate-0.4.3 portalocker-2.10.1 sacrebleu-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nsacrebleu_metric = evaluate.load(\"sacrebleu\")\nrouge_metric = evaluate.load('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:20:01.132655Z","iopub.execute_input":"2024-09-15T08:20:01.133024Z","iopub.status.idle":"2024-09-15T08:20:02.871271Z","shell.execute_reply.started":"2024-09-15T08:20:01.132987Z","shell.execute_reply":"2024-09-15T08:20:02.870154Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838e5df88e3342038cc5a323b8e68789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5abc06ea31ef463baf3dd38a7fe99a46"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade datasets","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:28:33.975909Z","iopub.execute_input":"2024-09-15T08:28:33.976810Z","iopub.status.idle":"2024-09-15T08:28:47.645866Z","shell.execute_reply.started":"2024-09-15T08:28:33.976756Z","shell.execute_reply":"2024-09-15T08:28:47.644706Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\n# Load the ROUGE metric\nrouge = evaluate.load(\"rouge\")\n\n# Assuming test_predictions and test_dataset are defined\npredictions = [item['predictions'] for item in test_predictions]\nreferences = [tokenizer.decode(item['labels'], skip_special_tokens=True) for item in test_dataset]\n\n# Compute ROUGE scores\nresults = rouge.compute(predictions=predictions, references=references)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:30:29.354221Z","iopub.execute_input":"2024-09-15T08:30:29.354742Z","iopub.status.idle":"2024-09-15T08:30:34.266761Z","shell.execute_reply.started":"2024-09-15T08:30:29.354693Z","shell.execute_reply":"2024-09-15T08:30:34.265606Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"{'rouge1': 0.7937453791851754, 'rouge2': 0.7830699181992671, 'rougeL': 0.7933683815253593, 'rougeLsum': 0.7929372513021002}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef question_answer(question, max_new_tokens=50):\n    model.eval()\n    \n    # Include instruction to generate only the answer\n    prompt = f\"Question: {question} Answer:\"\n    \n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs.get('attention_mask', None).to(device)\n\n    try:\n        output = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=input_ids.size(1) + max_new_tokens,\n            pad_token_id=tokenizer.pad_token_id,\n            temperature=1.0,  # Adjust temperature for diversity\n            top_k=50,\n            top_p=0.95\n        )\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Strip the question and keep only the answer part\n        generated_answer = generated_text.replace(f\"Question: {question} Answer:\", \"\").strip()\n        return generated_answer\n    except Exception as e:\n        print(f\"Error during text generation: {e}\")\n        return \"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:32:40.754798Z","iopub.execute_input":"2024-09-15T08:32:40.755558Z","iopub.status.idle":"2024-09-15T08:32:40.765925Z","shell.execute_reply.started":"2024-09-15T08:32:40.755518Z","shell.execute_reply":"2024-09-15T08:32:40.764799Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Move model to the device\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:10.634401Z","iopub.execute_input":"2024-09-15T08:33:10.634828Z","iopub.status.idle":"2024-09-15T08:33:10.803713Z","shell.execute_reply.started":"2024-09-15T08:33:10.634787Z","shell.execute_reply":"2024-09-15T08:33:10.802636Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('In machine learning, when is a tanh kernel typically utilized?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:27.595025Z","iopub.execute_input":"2024-09-15T08:33:27.595805Z","iopub.status.idle":"2024-09-15T08:33:27.708396Z","shell.execute_reply.started":"2024-09-15T08:33:27.595763Z","shell.execute_reply":"2024-09-15T08:33:27.706844Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'Yes, it is used for classification tasks.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('by adding layers, are we not creating overfitting?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:28.813832Z","iopub.execute_input":"2024-09-15T08:33:28.814207Z","iopub.status.idle":"2024-09-15T08:33:28.967867Z","shell.execute_reply.started":"2024-09-15T08:33:28.814172Z","shell.execute_reply":"2024-09-15T08:33:28.966777Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'Yes, by adding layers, are we not creating overfitting?'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :is Auto Encoder useful for dimensionality reduction of a numerical data set?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:34.673973Z","iopub.execute_input":"2024-09-15T08:33:34.674845Z","iopub.status.idle":"2024-09-15T08:33:34.852331Z","shell.execute_reply.started":"2024-09-15T08:33:34.674802Z","shell.execute_reply":"2024-09-15T08:33:34.851336Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'Yes, it can be used for dimensionality reduction of a numerical data set.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :Can we generate a sentence by using all the n-grams upto the length of sentence?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:37.878705Z","iopub.execute_input":"2024-09-15T08:33:37.879080Z","iopub.status.idle":"2024-09-15T08:33:37.957803Z","shell.execute_reply.started":"2024-09-15T08:33:37.879046Z","shell.execute_reply":"2024-09-15T08:33:37.956748Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'Yes, we can.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :What does DNN stand for in the context of machine learning?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:42.148764Z","iopub.execute_input":"2024-09-15T08:33:42.149661Z","iopub.status.idle":"2024-09-15T08:33:42.411723Z","shell.execute_reply.started":"2024-09-15T08:33:42.149619Z","shell.execute_reply":"2024-09-15T08:33:42.410713Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'DNN stands for Deep Neural Network, a type of neural network that is used in machine learning to learn from data.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Is RNN using human assistance for backpropagation?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:44.794031Z","iopub.execute_input":"2024-09-15T08:33:44.794977Z","iopub.status.idle":"2024-09-15T08:33:44.984036Z","shell.execute_reply.started":"2024-09-15T08:33:44.794933Z","shell.execute_reply":"2024-09-15T08:33:44.982733Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'Yes, it is possible to use RNN for backpropagation.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Why is slicing important and when is it used?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:47.794291Z","iopub.execute_input":"2024-09-15T08:33:47.794702Z","iopub.status.idle":"2024-09-15T08:33:47.925739Z","shell.execute_reply.started":"2024-09-15T08:33:47.794644Z","shell.execute_reply":"2024-09-15T08:33:47.924701Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'It is used to extract features from images or text.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}