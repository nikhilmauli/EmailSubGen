{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9365100,"sourceType":"datasetVersion","datasetId":5678899}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install chardet","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:41.445509Z","iopub.execute_input":"2024-09-15T07:39:41.445858Z","iopub.status.idle":"2024-09-15T07:39:56.818002Z","shell.execute_reply.started":"2024-09-15T07:39:41.445822Z","shell.execute_reply":"2024-09-15T07:39:56.816985Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting chardet\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: chardet\nSuccessfully installed chardet-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ndef convert_csv_to_text(input_csv, output_txt):\n    # Load CSV without headers and handle encoding issues\n    try:\n        df = pd.read_csv(input_csv, header=None, encoding='ISO-8859-1')  # Handle special characters\n    except pd.errors.ParserError:\n        # Try loading with tab separator if default fails\n        df = pd.read_csv(input_csv, header=None, delimiter='\\t', encoding='ISO-8859-1')\n\n    print(df.head())  # Debugging: Show the first few rows to check the structure\n\n    with open(output_txt, 'w', encoding='utf-8') as f:  # Ensure the output is UTF-8\n        current_question = None\n        for index, row in df.iterrows():\n            try:\n                # Convert each value to string and handle NaN values\n                question = str(row[0]) if not pd.isna(row[0]) else ''\n                answer = str(row[1]) if not pd.isna(row[1]) else ''\n\n                # Only proceed if both question and answer are non-empty\n                if question and answer:\n                    # Remove 'Question :' and 'Answer :' prefixes\n                    question = question.replace('Question :', '').strip()\n                    answer = answer.replace('Answer :', '').strip()\n\n                    if question == current_question:\n                        f.write(f\"Answer: {answer}\\n\")\n                    else:\n                        current_question = question\n                        f.write(f\"Question: {question}\\n\")\n                        f.write(f\"Answer: {answer}\\n\")\n            except KeyError as e:\n                print(f\"Error accessing row data: {e}\")\n                print(row)\n\n# Convert CSV files\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_train_1.csv', '/kaggle/working/train.txt')\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_dev_1.csv', '/kaggle/working/val.txt')\nconvert_csv_to_text('/kaggle/input/qa-dataset/aiml_qa_test_1.csv', '/kaggle/working/test.txt')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:56.820153Z","iopub.execute_input":"2024-09-15T07:39:56.820440Z","iopub.status.idle":"2024-09-15T07:39:57.379937Z","shell.execute_reply.started":"2024-09-15T07:39:56.820408Z","shell.execute_reply":"2024-09-15T07:39:57.378722Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"                                                   0  \\\n0  Question :What is the difference between conca...   \n1  Question :What is the difference between conca...   \n2  Question :Why are derivatives substracted from...   \n3  Question :Why are derivatives substracted from...   \n4  Question :Describe a process/pipeline for gene...   \n\n                                                   1  \n0  Answer :Concatenation combines two tensors by ...  \n1  Answer :Concatenation is often used to combine...  \n2  Answer :The derivative of the loss function at...  \n3  Answer :The intuition behind adjusting the wei...  \n4  Answer :Select a pretrained model suitable for...  \n                                                   0  \\\n0  Question : Does the maximum value of 'n' in th...   \n1                          Question : How CNN works?   \n2  Question : How is NMT trained? Is it common to...   \n3  Question : What is the process of learning POS...   \n4  Question : how to handle multi lingual situati...   \n\n                                                   1  \\\n0  Answer1 :The maximum value of n in an n-gram m...   \n1  Answer1 :Convolutional Neural Networks (CNNs) ...   \n2  Answer1 :Neural Machine Translation (NMT) is t...   \n3  Answer1 :The process of learning POS tags invo...   \n4  Answer1 :Handling multilingual situations in N...   \n\n                                                   2  \n0  Answer2 :The maximum value of 'n' in an n-gram...  \n1  Answer2 :CNN is a type of neural network that ...  \n2  Answer2 :Yes, training NMT models involves usi...  \n3  Answer2 :POS tags are learned by training mach...  \n4  Answer2 :It involves techniques such as langua...  \n                                                   0  \\\n0  Question :How we can effectively convert 2D im...   \n1  Question :Can we utilize an autoencoder to per...   \n2  Question :What is NLP's current biggest challe...   \n3  Question :Which problems cannot be solved by N...   \n4            Question :Is scaling necessary for SVM?   \n\n                                                   1  \\\n0  Answer1 :Converting images to 1D data may not ...   \n1  Answer1 :Yes, autoencoders can be applied to n...   \n2  Answer1 :The main challenges of NLP is finding...   \n3  Answer1 :While neural networks have shown grea...   \n4  Answer1 :Yes, scaling the input data is genera...   \n\n                                                   2  \n0  Answer2 :To effectively convert 2D images to 1...  \n1  Answer2 :Yes, autoencoders can be used for dim...  \n2  Answer2 :NLP models struggle with tasks that r...  \n3  Answer2 :Neural networks are powerful, but the...  \n4  Answer2 :Scaling the input data is advisable w...  \n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\ndef augment_data(input_csv, output_csv):\n    df = pd.read_csv(input_csv, header=None, encoding='ISO-8859-1')\n    augmented_rows = []\n    \n    for _, row in df.iterrows():\n        question = row[0]\n        answers = [row[i] for i in range(1, len(row)) if pd.notna(row[i])]\n        \n        if len(answers) > 1:\n            augmented_answers = [answer.replace('Answer:', 'Paraphrased Answer:') for answer in answers]\n            for ans in augmented_answers:\n                augmented_rows.append([question] + [ans])\n    \n    augmented_df = pd.DataFrame(augmented_rows, columns=['Question', 'Answer'])\n    augmented_df.to_csv(output_csv, index=False, header=False)\n\n# Augment the training data\naugment_data('/kaggle/input/qa-dataset/aiml_qa_train_1.csv', '/kaggle/working/augmented_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:57.381108Z","iopub.execute_input":"2024-09-15T07:39:57.381387Z","iopub.status.idle":"2024-09-15T07:39:57.500824Z","shell.execute_reply.started":"2024-09-15T07:39:57.381356Z","shell.execute_reply":"2024-09-15T07:39:57.499955Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade datasets transformers","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:39:57.503739Z","iopub.execute_input":"2024-09-15T07:39:57.504570Z","iopub.status.idle":"2024-09-15T07:40:23.942839Z","shell.execute_reply.started":"2024-09-15T07:39:57.504536Z","shell.execute_reply":"2024-09-15T07:40:23.941665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting datasets\n  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nCollecting transformers\n  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m665.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-3.0.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, datasets\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.0\n    Uninstalling transformers-4.44.0:\n      Successfully uninstalled transformers-4.44.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.21.0\n    Uninstalling datasets-2.21.0:\n      Successfully uninstalled datasets-2.21.0\nSuccessfully installed datasets-3.0.0 transformers-4.44.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# Load the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\ndef tokenize_function(data):\n    encodings = tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=512)\n    encodings['labels'] = encodings['input_ids']\n    return encodings\n\n# Load and tokenize datasets\ntrain_dataset = load_dataset('text', data_files='/kaggle/working/train.txt')['train']\nval_dataset = load_dataset('text', data_files='/kaggle/working/val.txt')['train']\ntest_dataset = load_dataset('text', data_files='/kaggle/working/test.txt')['train']\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nval_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:40:23.954938Z","iopub.execute_input":"2024-09-15T07:40:23.955192Z","iopub.status.idle":"2024-09-15T07:40:54.910243Z","shell.execute_reply.started":"2024-09-15T07:40:23.955164Z","shell.execute_reply":"2024-09-15T07:40:54.909436Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a6d8f626bfe4bcca08bc583523c7328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c600dd8c18241a8806309498d6a7324"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504cee2e87dd483885c64ed5b7073178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17e57c874f64f90a4498f4278d21be6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651164c888324be9b4ba315962631f6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee30a5e43b5941e9a61aa38627c9c11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cc057971be44e296d31580eedb6615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c594460b59f4b06afbd6440d749e267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1dd6dd2c8a4abfb8ce57eef1fe0351"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76fbb867348f4905be3c193e655dfe55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1091 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54daf5e7d931473eba4523a63d1bb940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f0d310cc8d490b9699c325228ae35f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42c7a8ff24a41d3a70365eb9aedacd1"}},"metadata":{}}]},{"cell_type":"code","source":"# Define training arguments with adjusted parameters\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/output\",\n    evaluation_strategy=\"epoch\",\n    num_train_epochs=5,          # Experiment with fewer epochs\n    learning_rate=5e-5,          # Adjust learning rate\n    per_device_train_batch_size=8,  # Adjust batch size\n    per_device_eval_batch_size=8,   # Adjust batch size\n    logging_dir=\"/kaggle/working/logs\",\n    save_total_limit=1,\n    save_steps=500,             # Save less frequently\n    logging_steps=50,           # Log more frequently\n    warmup_steps=500,           # Adjust warmup steps\n    weight_decay=0.01,\n    max_grad_norm=1.0,          # Gradient clipping\n    fp16=True,                  # Use mixed precision if supported\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\n# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:40:54.911276Z","iopub.execute_input":"2024-09-15T07:40:54.911884Z","iopub.status.idle":"2024-09-15T07:50:51.777510Z","shell.execute_reply.started":"2024-09-15T07:40:54.911847Z","shell.execute_reply":"2024-09-15T07:50:51.776592Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240915_074116-5q038qth</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth' target=\"_blank\">/kaggle/working/output</a></strong> to <a href='https://wandb.ai/abcdkgf993-aiml/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/abcdkgf993-aiml/huggingface' target=\"_blank\">https://wandb.ai/abcdkgf993-aiml/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth' target=\"_blank\">https://wandb.ai/abcdkgf993-aiml/huggingface/runs/5q038qth</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [345/345 09:15, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.914400</td>\n      <td>0.246059</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.270700</td>\n      <td>0.204762</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.158800</td>\n      <td>0.199218</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.145500</td>\n      <td>0.196333</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.143800</td>\n      <td>0.195846</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=345, training_loss=1.0048119876695716, metrics={'train_runtime': 593.6304, 'train_samples_per_second': 9.189, 'train_steps_per_second': 0.581, 'total_flos': 1425348034560000.0, 'train_loss': 1.0048119876695716, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"/kaggle/working/saved_model\")\ntokenizer.save_pretrained(\"/kaggle/working/saved_model\")","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:50:51.779081Z","iopub.execute_input":"2024-09-15T07:50:51.779690Z","iopub.status.idle":"2024-09-15T07:50:53.126413Z","shell.execute_reply.started":"2024-09-15T07:50:51.779637Z","shell.execute_reply":"2024-09-15T07:50:53.125318Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/saved_model/tokenizer_config.json',\n '/kaggle/working/saved_model/special_tokens_map.json',\n '/kaggle/working/saved_model/vocab.json',\n '/kaggle/working/saved_model/merges.txt',\n '/kaggle/working/saved_model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r /kaggle/working/saved_model_qa.zip /kaggle/working/saved_model","metadata":{"execution":{"iopub.status.busy":"2024-09-15T07:50:53.127991Z","iopub.execute_input":"2024-09-15T07:50:53.128446Z","iopub.status.idle":"2024-09-15T07:51:21.345105Z","shell.execute_reply.started":"2024-09-15T07:50:53.128395Z","shell.execute_reply":"2024-09-15T07:51:21.343958Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/saved_model/ (stored 0%)\n  adding: kaggle/working/saved_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/saved_model/config.json (deflated 52%)\n  adding: kaggle/working/saved_model/generation_config.json (deflated 24%)\n  adding: kaggle/working/saved_model/merges.txt (deflated 53%)\n  adding: kaggle/working/saved_model/vocab.json (deflated 68%)\n  adding: kaggle/working/saved_model/special_tokens_map.json (deflated 74%)\n  adding: kaggle/working/saved_model/tokenizer_config.json (deflated 55%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom datasets import load_dataset\n\n# Load the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/saved_model')\nmodel = GPT2LMHeadModel.from_pretrained('/kaggle/working/saved_model')\n\n# Load the test dataset\ntest_dataset = load_dataset('text', data_files='/kaggle/working/test.txt')['train']\n\n# Print column names to understand the dataset structure\nprint(test_dataset.column_names)\n\n# Tokenize the test dataset\ndef tokenize_function(data):\n    encodings = tokenizer(data['text'], padding=\"max_length\", truncation=True, max_length=512)\n    encodings['labels'] = encodings['input_ids']\n    return encodings\n\ntest_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:08:45.284421Z","iopub.execute_input":"2024-09-15T08:08:45.285222Z","iopub.status.idle":"2024-09-15T08:08:48.033204Z","shell.execute_reply.started":"2024-09-15T08:08:45.285180Z","shell.execute_reply":"2024-09-15T08:08:48.032192Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['text']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fe962326ef4b96ab375c6fb3f98a0c"}},"metadata":{}}]},{"cell_type":"code","source":"print(test_dataset.column_names)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:08:53.123740Z","iopub.execute_input":"2024-09-15T08:08:53.124116Z","iopub.status.idle":"2024-09-15T08:08:53.131215Z","shell.execute_reply.started":"2024-09-15T08:08:53.124081Z","shell.execute_reply":"2024-09-15T08:08:53.130030Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom datasets import load_dataset\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:03.558976Z","iopub.execute_input":"2024-09-15T08:09:03.559405Z","iopub.status.idle":"2024-09-15T08:09:03.566623Z","shell.execute_reply.started":"2024-09-15T08:09:03.559352Z","shell.execute_reply":"2024-09-15T08:09:03.565649Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def predict(example):\n    input_ids = torch.tensor(example['input_ids']).unsqueeze(0)  # Add batch dimension\n    attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0)  # Add batch dimension\n\n    prediction = generate_predictions(input_ids, attention_mask, model, tokenizer)\n    return {\"predictions\": prediction}\n\ndef generate_predictions(input_ids, attention_mask, model, tokenizer, max_new_tokens=50):\n    outputs = model.generate(\n        input_ids,\n        attention_mask=attention_mask,  # Include attention mask\n        max_new_tokens=max_new_tokens,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2\n    )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:07.199848Z","iopub.execute_input":"2024-09-15T08:09:07.200781Z","iopub.status.idle":"2024-09-15T08:09:07.210793Z","shell.execute_reply.started":"2024-09-15T08:09:07.200661Z","shell.execute_reply":"2024-09-15T08:09:07.209468Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Apply prediction function to the test dataset\ntest_predictions = test_dataset.map(predict, batched=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:09:11.879861Z","iopub.execute_input":"2024-09-15T08:09:11.880249Z","iopub.status.idle":"2024-09-15T08:15:08.003855Z","shell.execute_reply.started":"2024-09-15T08:09:11.880212Z","shell.execute_reply":"2024-09-15T08:15:08.002855Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc57b00864e40a599c33148d5db3b8e"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:15:08.005940Z","iopub.execute_input":"2024-09-15T08:15:08.006631Z","iopub.status.idle":"2024-09-15T08:15:23.943562Z","shell.execute_reply.started":"2024-09-15T08:15:08.006579Z","shell.execute_reply":"2024-09-15T08:15:23.942352Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b46d53143ea3b5c13ecc633eaf160b91ea91ee091155a19326ce8aec2d3797fe\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:19:46.239573Z","iopub.execute_input":"2024-09-15T08:19:46.240042Z","iopub.status.idle":"2024-09-15T08:20:01.130416Z","shell.execute_reply.started":"2024-09-15T08:19:46.239999Z","shell.execute_reply":"2024-09-15T08:20:01.129123Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nCollecting sacrebleu\n  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m754.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.5.15)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nInstalling collected packages: portalocker, sacrebleu, evaluate\nSuccessfully installed evaluate-0.4.3 portalocker-2.10.1 sacrebleu-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nsacrebleu_metric = evaluate.load(\"sacrebleu\")\nrouge_metric = evaluate.load('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:20:01.132655Z","iopub.execute_input":"2024-09-15T08:20:01.133024Z","iopub.status.idle":"2024-09-15T08:20:02.871271Z","shell.execute_reply.started":"2024-09-15T08:20:01.132987Z","shell.execute_reply":"2024-09-15T08:20:02.870154Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838e5df88e3342038cc5a323b8e68789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5abc06ea31ef463baf3dd38a7fe99a46"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade datasets","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:28:33.975909Z","iopub.execute_input":"2024-09-15T08:28:33.976810Z","iopub.status.idle":"2024-09-15T08:28:47.645866Z","shell.execute_reply.started":"2024-09-15T08:28:33.976756Z","shell.execute_reply":"2024-09-15T08:28:47.644706Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\n# Load the ROUGE metric\nrouge = evaluate.load(\"rouge\")\n\n# Assuming test_predictions and test_dataset are defined\npredictions = [item['predictions'] for item in test_predictions]\nreferences = [tokenizer.decode(item['labels'], skip_special_tokens=True) for item in test_dataset]\n\n# Compute ROUGE scores\nresults = rouge.compute(predictions=predictions, references=references)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:30:29.354221Z","iopub.execute_input":"2024-09-15T08:30:29.354742Z","iopub.status.idle":"2024-09-15T08:30:34.266761Z","shell.execute_reply.started":"2024-09-15T08:30:29.354693Z","shell.execute_reply":"2024-09-15T08:30:34.265606Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"{'rouge1': 0.7937453791851754, 'rouge2': 0.7830699181992671, 'rougeL': 0.7933683815253593, 'rougeLsum': 0.7929372513021002}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef question_answer(question, max_new_tokens=50):\n    model.eval()\n    \n    # Include instruction to generate only the answer\n    prompt = f\"Question: {question} Answer:\"\n    \n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs.get('attention_mask', None).to(device)\n\n    try:\n        output = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=input_ids.size(1) + max_new_tokens,\n            pad_token_id=tokenizer.pad_token_id,\n            temperature=1.0,  # Adjust temperature for diversity\n            top_k=50,\n            top_p=0.95\n        )\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Strip the question and keep only the answer part\n        generated_answer = generated_text.replace(f\"Question: {question} Answer:\", \"\").strip()\n        return generated_answer\n    except Exception as e:\n        print(f\"Error during text generation: {e}\")\n        return \"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:32:40.754798Z","iopub.execute_input":"2024-09-15T08:32:40.755558Z","iopub.status.idle":"2024-09-15T08:32:40.765925Z","shell.execute_reply.started":"2024-09-15T08:32:40.755518Z","shell.execute_reply":"2024-09-15T08:32:40.764799Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Move model to the device\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:10.634401Z","iopub.execute_input":"2024-09-15T08:33:10.634828Z","iopub.status.idle":"2024-09-15T08:33:10.803713Z","shell.execute_reply.started":"2024-09-15T08:33:10.634787Z","shell.execute_reply":"2024-09-15T08:33:10.802636Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('In machine learning, when is a tanh kernel typically utilized?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:27.595025Z","iopub.execute_input":"2024-09-15T08:33:27.595805Z","iopub.status.idle":"2024-09-15T08:33:27.708396Z","shell.execute_reply.started":"2024-09-15T08:33:27.595763Z","shell.execute_reply":"2024-09-15T08:33:27.706844Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'Yes, it is used for classification tasks.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('by adding layers, are we not creating overfitting?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:28.813832Z","iopub.execute_input":"2024-09-15T08:33:28.814207Z","iopub.status.idle":"2024-09-15T08:33:28.967867Z","shell.execute_reply.started":"2024-09-15T08:33:28.814172Z","shell.execute_reply":"2024-09-15T08:33:28.966777Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"'Yes, by adding layers, are we not creating overfitting?'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :is Auto Encoder useful for dimensionality reduction of a numerical data set?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:34.673973Z","iopub.execute_input":"2024-09-15T08:33:34.674845Z","iopub.status.idle":"2024-09-15T08:33:34.852331Z","shell.execute_reply.started":"2024-09-15T08:33:34.674802Z","shell.execute_reply":"2024-09-15T08:33:34.851336Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'Yes, it can be used for dimensionality reduction of a numerical data set.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :Can we generate a sentence by using all the n-grams upto the length of sentence?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:37.878705Z","iopub.execute_input":"2024-09-15T08:33:37.879080Z","iopub.status.idle":"2024-09-15T08:33:37.957803Z","shell.execute_reply.started":"2024-09-15T08:33:37.879046Z","shell.execute_reply":"2024-09-15T08:33:37.956748Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'Yes, we can.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Question :What does DNN stand for in the context of machine learning?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:42.148764Z","iopub.execute_input":"2024-09-15T08:33:42.149661Z","iopub.status.idle":"2024-09-15T08:33:42.411723Z","shell.execute_reply.started":"2024-09-15T08:33:42.149619Z","shell.execute_reply":"2024-09-15T08:33:42.410713Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'DNN stands for Deep Neural Network, a type of neural network that is used in machine learning to learn from data.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Is RNN using human assistance for backpropagation?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:44.794031Z","iopub.execute_input":"2024-09-15T08:33:44.794977Z","iopub.status.idle":"2024-09-15T08:33:44.984036Z","shell.execute_reply.started":"2024-09-15T08:33:44.794933Z","shell.execute_reply":"2024-09-15T08:33:44.982733Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'Yes, it is possible to use RNN for backpropagation.'"},"metadata":{}}]},{"cell_type":"code","source":"question_answer('Why is slicing important and when is it used?')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T08:33:47.794291Z","iopub.execute_input":"2024-09-15T08:33:47.794702Z","iopub.status.idle":"2024-09-15T08:33:47.925739Z","shell.execute_reply.started":"2024-09-15T08:33:47.794644Z","shell.execute_reply":"2024-09-15T08:33:47.924701Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"'It is used to extract features from images or text.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}