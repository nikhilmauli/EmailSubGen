{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9056495,"sourceType":"datasetVersion","datasetId":5460856}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T09:33:08.892748Z","iopub.execute_input":"2024-07-29T09:33:08.893095Z","iopub.status.idle":"2024-07-29T09:33:10.923612Z","shell.execute_reply.started":"2024-07-29T09:33:08.893064Z","shell.execute_reply":"2024-07-29T09:33:10.922672Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/subjects/EmailSubjectTest.txt\n/kaggle/input/subjects/EmailSubjectTrain.txt\n","output_type":"stream"}]},{"cell_type":"code","source":" !pip install accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:33:10.925130Z","iopub.execute_input":"2024-07-29T09:33:10.925520Z","iopub.status.idle":"2024-07-29T09:33:27.386960Z","shell.execute_reply.started":"2024-07-29T09:33:10.925494Z","shell.execute_reply":"2024-07-29T09:33:27.386010Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting accelerate\n  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\nSuccessfully installed accelerate-0.33.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk==3.8.1 -U","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:33:27.388945Z","iopub.execute_input":"2024-07-29T09:33:27.389354Z","iopub.status.idle":"2024-07-29T09:33:42.330716Z","shell.execute_reply.started":"2024-07-29T09:33:27.389316Z","shell.execute_reply":"2024-07-29T09:33:42.329562Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting nltk==3.8.1\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.8.1) (4.66.4)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/ryanzhumich/AESLC","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:33:42.333751Z","iopub.execute_input":"2024-07-29T09:33:42.334200Z","iopub.status.idle":"2024-07-29T09:33:45.464521Z","shell.execute_reply.started":"2024-07-29T09:33:42.334162Z","shell.execute_reply":"2024-07-29T09:33:45.463586Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'AESLC'...\nremote: Enumerating objects: 17469, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 17469 (delta 1), reused 0 (delta 0), pack-reused 17461\u001b[K\nReceiving objects: 100% (17469/17469), 7.36 MiB | 17.32 MiB/s, done.\nResolving deltas: 100% (48/48), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport nltk\n# nltk.download()\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:33:45.465883Z","iopub.execute_input":"2024-07-29T09:33:45.466192Z","iopub.status.idle":"2024-07-29T09:34:14.892091Z","shell.execute_reply.started":"2024-07-29T09:33:45.466163Z","shell.execute_reply":"2024-07-29T09:34:14.891246Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-29 09:33:59.339760: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-29 09:33:59.339898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-29 09:33:59.643734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ensure NLTK resources are downloaded\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:14.893186Z","iopub.execute_input":"2024-07-29T09:34:14.893757Z","iopub.status.idle":"2024-07-29T09:34:15.250904Z","shell.execute_reply.started":"2024-07-29T09:34:14.893729Z","shell.execute_reply":"2024-07-29T09:34:15.250057Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Function for text preprocessing\ndef preprocess_text(text):\n    # Tokenize text\n    tokens = word_tokenize(text.lower())\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words and word.isalnum()]\n\n    # Lemmatize tokens\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n    # Join tokens back into a string\n    preprocessed_text = ' '.join(tokens)\n\n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:15.252346Z","iopub.execute_input":"2024-07-29T09:34:15.252975Z","iopub.status.idle":"2024-07-29T09:34:15.259455Z","shell.execute_reply.started":"2024-07-29T09:34:15.252939Z","shell.execute_reply":"2024-07-29T09:34:15.258313Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Custom dataset class\nclass EmailSubjectDataset(Dataset):\n    def __init__(self, emails, subjects, tokenizer, max_length):\n        self.emails = emails\n        self.subjects = subjects\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.emails)\n\n    def __getitem__(self, idx):\n        email_text = self.emails[idx]\n        subject_text = self.subjects[idx]\n\n        # Tokenize inputs and outputs\n        inputs = self.tokenizer(email_text, return_tensors='pt', max_length=self.max_length, truncation=True,\n                                padding='max_length')\n        outputs = self.tokenizer(subject_text, return_tensors='pt', max_length=self.max_length, truncation=True,\n                                 padding='max_length')\n\n        return {\n            'input_ids': inputs.input_ids.flatten(),\n            'attention_mask': inputs.attention_mask.flatten(),\n            'labels': outputs.input_ids.flatten()\n        }","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:15.260723Z","iopub.execute_input":"2024-07-29T09:34:15.261453Z","iopub.status.idle":"2024-07-29T09:34:15.269524Z","shell.execute_reply.started":"2024-07-29T09:34:15.261420Z","shell.execute_reply":"2024-07-29T09:34:15.268512Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Directory containing email files\nemails_directory = '/kaggle/working/AESLC/enron_subject_line/train'\n\n# Initialize tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:15.270653Z","iopub.execute_input":"2024-07-29T09:34:15.270909Z","iopub.status.idle":"2024-07-29T09:34:16.674487Z","shell.execute_reply.started":"2024-07-29T09:34:15.270887Z","shell.execute_reply":"2024-07-29T09:34:16.673701Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fffbb3acba48b68328f3178af304ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64336b1cf324ae38ce6ef6c508f7d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f248e3f71b04964a3b2643c2ef02501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9630e3bb6ccf4a0c965ca5f21c45720d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d420c409f14a2084baed1c86b04cbe"}},"metadata":{}}]},{"cell_type":"code","source":"# Load emails and subjects from files\nemails = []\nsubjects = []\n\nfor filename in os.listdir(emails_directory):\n\n    with open(os.path.join(emails_directory, filename), 'r', encoding='utf-8', errors='ignore') as file:\n        email_text = file.read()\n        # Extract subject from email body using regex\n        match = re.search(r'@subject\\s+(.*)', email_text, re.IGNORECASE | re.DOTALL)\n\n        if match:\n            subject = match.group(1).strip()\n            email_body = re.sub(r'@subject\\n(.+)\\n', '', email_text).strip()\n            preprocessed_email = preprocess_text(email_body)\n\n            emails.append(preprocessed_email)\n            subjects.append(subject)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:16.677165Z","iopub.execute_input":"2024-07-29T09:34:16.677460Z","iopub.status.idle":"2024-07-29T09:34:53.531562Z","shell.execute_reply.started":"2024-07-29T09:34:16.677435Z","shell.execute_reply":"2024-07-29T09:34:53.530755Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create dataset\nprint(emails[0])\nprint(subjects[0])\ndataset = EmailSubjectDataset(emails, subjects, tokenizer, max_length=512)\n\n# Split dataset into training and validation sets\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:53.532634Z","iopub.execute_input":"2024-07-29T09:34:53.532917Z","iopub.status.idle":"2024-07-29T09:34:53.574182Z","shell.execute_reply.started":"2024-07-29T09:34:53.532892Z","shell.execute_reply":"2024-07-29T09:34:53.573352Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"greeting folk heard ken lay met california ceo last friday back east result meeting mcnealy ceo sun asked background detail ken lay proposed solution california meeting california past couple week mcnealy others expressed willingness make phone call policymakers etc try advance ball pulled together hastily friday take responsibility error omission ramblings etc wanted distribute however make sure info getting distributed know folk busy likely get distributed ceo week comment suggestion etc improve welcome appreciated best subject backgrounder mcnealy\nBackgrounder for McNealy\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    logging_dir='./logs',\n    logging_steps=500,\n    evaluation_strategy=\"epoch\",\n)\n\n# Initialize model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n)\n\n# Fine-tuning the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:34:53.575318Z","iopub.execute_input":"2024-07-29T09:34:53.575650Z","iopub.status.idle":"2024-07-29T10:39:49.865869Z","shell.execute_reply.started":"2024-07-29T09:34:53.575625Z","shell.execute_reply":"2024-07-29T10:39:49.865016Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea265d05d67c4f2ab5022a3080966bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4266e52f4c746be9411d3bbc57da502"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240729_093521-cu5wauhg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/nikhilchandramauli-npci/huggingface/runs/cu5wauhg' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/nikhilchandramauli-npci/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/nikhilchandramauli-npci/huggingface' target=\"_blank\">https://wandb.ai/nikhilchandramauli-npci/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/nikhilchandramauli-npci/huggingface/runs/cu5wauhg' target=\"_blank\">https://wandb.ai/nikhilchandramauli-npci/huggingface/runs/cu5wauhg</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4332' max='4332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4332/4332 1:04:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.096800</td>\n      <td>0.093909</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.090000</td>\n      <td>0.091731</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.086300</td>\n      <td>0.091299</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4332, training_loss=0.09385014020615118, metrics={'train_runtime': 3889.7176, 'train_samples_per_second': 8.907, 'train_steps_per_second': 1.114, 'total_flos': 9052201156608000.0, 'train_loss': 0.09385014020615118, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Function to generate subject lines from email text\ndef get_first_four_words_split(text):\n    words = text.split()\n    return ' '.join(words[:4])\n\n\n\n\ndef generate_subject(model, tokenizer, email_text, max_length=512):\n    # Move model to correct device\n    device = next(model.parameters()).device\n\n    email_text = preprocess_text(email_text)\n\n    input_ids = tokenizer.encode(email_text, return_tensors='pt').to(device)  # Move input_ids to device\n\n    # Generate subject line\n    with torch.no_grad():\n        output = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n\n    generated_subject = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_subject\n\nexample_email = \"The following reports have been waiting for your approval for more than 4 days. Please review. Owner: James W Reitmeyer Report Name: JReitmeyer 10/24/01 Days In Mgr.Queue: 5\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))\n\nexample_email = \"All, The below Analyst & Associate recruiting dates require ENA participation at Manager level at above. In order to resource each of your departments it is important to have ENA's involvement and participation in the interviews and debrief sessions on Fantastic Friday and Super Saturday events. These de-brief sessions will allow you the opportunity to select candidates you wish to join your groups. The target is to assign potential candidates to business units and departments from the outset. As ENA has the highest percentage of A&A rotating in its business unit, the participation of ENA at interview should reflect this. Therefore, please encourage your direct reports and managers to participate in the below events in order to secure candidates for your business area. Associate Recruiting: Saturday November 3 Total - 70 Candidates for Interview Analyst Recruiting: Friday, November 16 Total - 70 Candidates for Interview Associate Recruiting: Saturday, December 1 Total - 70 Candidates for Interview The above spreadsheet represents ENA's particpation today which I believe highlights the need for much additional support in these efforts. Please confirm by return participation of your respective groups. Regards,\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))\n\nexample_email = \"Late on October 25th, we received information about a nonspecific threat to the Enron Center. We communicated with law enforcement officials who found the threat unsubstantiated and without merit. Nonetheless we take all threats seriously and have increased the security presence at the Enron Center still further. Once again, if you observe suspicious behavior, please call security at 3-6200.\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))\n\nexample_email = \"Thanks in advance for agreeing to speak at the Global Operations Controller Forum. There will be approximately 30 Enron business controllers present at the meeting. All have responsibility for mid and back office operations for the following Enron entities: Enron North America, Enron Europe, Enron South America, Enron Global Markets, Enron Industrial Markets, Enron Broadband Services and Enron Energy Services. Attendees will be here from Houston, Calgary, Tokyo, Sydney, London and New York (metals business). Attached for your reference is the agenda. There may be some slight changes before the forum begins, but this will give you a good idea of the topics to be covered and the other speakers who will address the group. You are scheduled to address the group as follows:\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))\n\nexample_email = \"To confirm: Mark Thomas Mike Presley both previously on the list for Netco's Market Risk/Research Group have resigned. I'm combing the Estate for replacements. DP\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))\n\nexample_email = \"Michelle; This is the presentation which was provided to the HR VP's. The HR VPs were tasked with going off to met their BU OTCs and devise plans around the three components. We are currently collating those plans into one document. We'll send that along to you when completed. Pls call if you have any questions.\"\ngenerated_subject = generate_subject(model, tokenizer, example_email)\nprint(\"Generated Subject:\", get_first_four_words_split(generated_subject))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T10:39:49.867324Z","iopub.execute_input":"2024-07-29T10:39:49.867678Z","iopub.status.idle":"2024-07-29T10:39:50.715655Z","shell.execute_reply.started":"2024-07-29T10:39:49.867645Z","shell.execute_reply":"2024-07-29T10:39:50.714645Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Subject: following report waiting approval\nGenerated Subject: analyst associate recruiting date\nGenerated Subject: late october 25th received\nGenerated Subject: thanks advance agreeing speak\nGenerated Subject: confirm mark thomas mike\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Subject: michelle presentation provided hr\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save model to local directory\nlocal_model_dir = './saved_model'\ntrainer.save_model(local_model_dir)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T11:12:04.789810Z","iopub.execute_input":"2024-07-29T11:12:04.790571Z","iopub.status.idle":"2024-07-29T11:12:05.849160Z","shell.execute_reply.started":"2024-07-29T11:12:04.790535Z","shell.execute_reply":"2024-07-29T11:12:05.848045Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(os.listdir('/kaggle/working'))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T11:08:22.740540Z","iopub.execute_input":"2024-07-29T11:08:22.741456Z","iopub.status.idle":"2024-07-29T11:08:22.748697Z","shell.execute_reply.started":"2024-07-29T11:08:22.741420Z","shell.execute_reply":"2024-07-29T11:08:22.747580Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"['AESLC', 'wandb', 'results', 'gpt2model.zip', 'finalmodel.zip', 'logs', '.virtual_documents', 'saved_model']\n","output_type":"stream"}]},{"cell_type":"code","source":"#!zip -r /kaggle/working/finalmodel.zip /kaggle/working/saved_model\n\nimport shutil\nimport os\n\nzip_filename = '/kaggle/working/finalmodel.zip'\nshutil.make_archive(zip_filename.split('.')[0], 'zip', local_model_dir)\n\n# Note: make_archive creates a ZIP file with the base name (finalmodel) in the current working directory (/kaggle/working/)\n\n# Clean up: Remove the original saved model directory\nshutil.rmtree(local_model_dir)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T11:08:27.282274Z","iopub.execute_input":"2024-07-29T11:08:27.282638Z","iopub.status.idle":"2024-07-29T11:08:56.594144Z","shell.execute_reply.started":"2024-07-29T11:08:27.282602Z","shell.execute_reply":"2024-07-29T11:08:56.593131Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Saving and downloading the model\n\nimport joblib\nimport zipfile\n\n# Assuming 'model' is your trained model object\n# Example: saving a trained model using joblib\njoblib.dump(model, '/kaggle/working/model.pkl')\n\n# Create a ZIP file containing the saved model\nwith zipfile.ZipFile('/kaggle/working/gpt2model.zip', 'w') as zipf:\n    zipf.write('/kaggle/working/model.pkl')\n\n# Optionally, remove the temporary model.pkl file\nimport os\nos.remove('/kaggle/working/model.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T11:09:11.989904Z","iopub.execute_input":"2024-07-29T11:09:11.990731Z","iopub.status.idle":"2024-07-29T11:09:14.632596Z","shell.execute_reply.started":"2024-07-29T11:09:11.990698Z","shell.execute_reply":"2024-07-29T11:09:14.631125Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Create a link to the file\nFileLink(r'/kaggle/working/gpt2model.zip')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T11:13:05.657721Z","iopub.execute_input":"2024-07-29T11:13:05.658461Z","iopub.status.idle":"2024-07-29T11:13:05.666686Z","shell.execute_reply.started":"2024-07-29T11:13:05.658431Z","shell.execute_reply":"2024-07-29T11:13:05.665774Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/gpt2model.zip","text/html":"<a href='/kaggle/working/gpt2model.zip' target='_blank'>/kaggle/working/gpt2model.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}